---
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
---


::: {style="text-align: center"}
## Improving the Measurement of Sensitive Survey Questions with Double List Experiments{.center background-color="#7A003C"}

&nbsp;


**Gustavo Diaz**   
Postdoctoral Fellow  
Department of Political Science  
McMaster University  
[gustavodiaz.org](https://gustavodiaz.org/)
:::

```{r include=FALSE}
library(knitr)


opts_chunk$set(fig.width=9, fig.height=5, 
               fig.retina = 3, collapse = TRUE)
```

```{r setup}
# Packages
library(tidyverse)
library(DeclareDesign)
library(kableExtra)

# ggplot global options
theme_set(theme_bw(base_size = 20))
```

```{r}
# Data prep
load("attn_rep.RData")

cali = dle

remove(dle)

# Analysis
a = cali %>% 
  split(.$experiment) %>% 
  map(~difference_in_means(listA ~ trt_A, data = .)) %>% 
  map(tidy) %>% 
  bind_rows(.id = "experiment")

b = cali %>% 
  split(.$experiment) %>% 
  map(~difference_in_means(listB ~ trt_B, data = .)) %>% 
  map(tidy) %>% 
  bind_rows(.id = "experiment")

pool = cali %>% 
  mutate(id = row_number()) %>% 
  pivot_longer(cols = c(listA, listB)) %>% 
  rename(list = name, count = value) %>% 
  mutate(Z = ifelse(trt_A == 1 & list == "listA" | trt_B == 1 & list == "listB",
                    1, 0)) %>% 
  split(.$experiment) %>% 
  map(~ lm_robust(count ~ Z + list, clusters = id, se_type = "stata", data = .)) %>% 
  map(tidy) %>% 
  bind_rows(.id = "experiment") %>% 
  filter(term == "Z")
  
est_df = rbind(a, b, pool)

est_df$term = fct_relevel(est_df$term, "trt_A", "trt_B", "Z")
```

## ArriveCAN

![](figs/canada_covid.png){fig-align="center"}

## COVID screening questions

![](figs/arrivecan.png){fig-align="center"}

## What proportion of app users answered "yes" to having symptoms?

. . .

- If you are **not** sick, you answer **no**

. . .

- If you are sick...

. . .

- You don't travel and don't use the app 

. . .

**OR**
    
. . .

- You lie about having symptoms and travel anyway

## From a social science perspective

::: incremental

- We use surveys a lot

- Many questions are **too sensitive** to ask directly

- But these are things we care about!

- **Examples:** Racial prejudice, support for rebel groups, sexual and reproductive behavior

- Many techniques use **indirect** questions to elicit **honest answers** while **protecting anonymity**

:::

## List experiments

Here is a list of things that some people have
done. Tell me HOW MANY of them you
have done in the past two years. Do not tell me
which ones. Just tell me
how many:

. . .

- Discussed politics with family or friends
- Cast a ballot for Governor Phil Bryant
- Paid dues to a union
- Given money to a Tea Party candidate or organization

::: footer
Rosenfeld, Imai, and Shapiro. 2016. "An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions." *American Journal of Political Science* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1111/ajps.12205)
:::

## List experiments {visibility="uncounted"}

Here is a list of things that some people have
done. Tell me HOW MANY of them you
have done in the past two years. Do not tell me
which ones. Just tell me
how many:

- Discussed politics with family or friends
- Cast a ballot for Governor Phil Bryant
- Paid dues to a union
- Given money to a Tea Party candidate or organization
- **Voted 'YES' on the 'Personhood' Initiative on the November 2011 Mississippi General Election ballot**

::: footer
Rosenfeld, Imai, and Shapiro. 2016. "An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions." *American Journal of Political Science* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1111/ajps.12205)
:::


## Prevalence rate

$$
\text{Prevalence(X)} = \text{Mean(List with X)} - \text{Mean(List without X)}
$$

. . .

- Proportion of individuals in the target population who hold the sensitive attitude or behavior

- We obtain an estimate while keeping individual responses anonymous 

## This reduces sensitivity bias

![](figs/validation_0.png){fig-align="center" width="50%" height="50%"}

::: footer
Rosenfeld, Imai, and Shapiro. 2016. "An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions." *American Journal of Political Science* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1111/ajps.12205)
:::

## This reduces sensitivity bias {visibility="uncounted"}

![](figs/validation_1.png){fig-align="center" width="50%" height="50%"}

::: footer
Rosenfeld, Imai, and Shapiro. 2016. "An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions." *American Journal of Political Science* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1111/ajps.12205)
:::

## But increases variance {visibility="uncounted"}

![](figs/validation_2.png){fig-align="center" width="50%" height="50%"}

::: footer
Rosenfeld, Imai, and Shapiro. 2016. "An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions." *American Journal of Political Science* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1111/ajps.12205)
:::

## Bias-variance tradeoff

![](figs/tradeoff.png){fig-align="center"}

. . .

{{< fa arrow-right >}} List experiments need large samples!

::: footer
Blair, Coppock, and Moor. 2020. "When to Worry about Sensitivity Bias: A Social Reference Theory and Evidence from 30 Years of List Experiments." *American Political Science Review* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1017/S0003055420000374)
:::

## To make matters worse

::: incremental
- More sensitive attitudes {{< fa arrow-right >}} more interesting {{< fa arrow-right >}} more uncommon {{< fa arrow-right >}} harder to detect

- Some sensitive attitudes are more prevalent among hard-to-reach populations

- Multiple sensitive attitudes increase sample size requirements

- Same when comparing subgroup effects

- **Very hard to make list experiments work**

:::

::: {style="text-align: center"}
## What to do? {.center background-color="#FDBF57"}
:::

## Double list experiments {.smaller}

**List A**

- Californians for Disability (advocating for people with disabilities)
- California National Organization for Women (advocating for women's equality and empowerment)
- American Family Association (advocating for pro-family values)
- American Red Cross (humanitarian organization)

**List B**

- American Legion (veterans service organization)
- Equality California (gay and lesbian advocacy organization)
- Tea Party Patriots (conservative group supporting lower taxes and limited government)
- Salvation Army (charitable organization)

::: footer
Alvarez, Atkeson, Levin, and Li. 2019. "Paying Attention to Inattentive Survey Respondents." *Political Analysis* [{{< fa arrow-up-right-from-square >}}](https://doi.org/10.1017/pan.2018.57)
:::

## Sensitive item

**Organization X (advocating for immigration reduction and measures against undocumented immigration)**

::: incremental
- Randomly appears in list A or B

- Equivalent to making two parallel list experiments

- Calculate prevalence by averaging the two single-list estimates
:::



## Variance reduced by half

```{r}
ggplot(est_df %>% filter(experiment == "X")) +
  aes(x = term, y = estimate, shape = term) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_linerange(aes(x = term, ymin = conf.low, ymax = conf.high), 
                 size = 1, position = position_dodge(width = 0.5)) +
  theme(legend.position = "none") +
  labs(subtitle = "Organization X", 
       x = "Estimator", 
       y = "Proportion support") +
  scale_x_discrete(labels = c("List A", "List B", "Pooled"))
```

## Challenge

::: incremental
- Variance reduction is not free!

- Need to craft two baseline lists

- This is costly can go wrong
:::

## Another sensitive item

**Organization Y (citizen border patrol group combating undocumented immigration)**

::: incremental
- Respondents randomly assigned to see X or Y
- Mutually exclusive {{< fa arrow-right >}} independent DLEs
- Everything else is the same...
:::

## Yet results are different!

```{r}
ggplot(est_df %>% filter(experiment == "Y")) +
  aes(x = term, y = estimate, shape = term) +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_linerange(aes(x = term, ymin = conf.low, ymax = conf.high), 
                 size = 1, position = position_dodge(width = 0.5)) +
  theme(legend.position = "none") +
  labs(subtitle = "Organization Y", 
       x = "Estimator", 
       y = "Proportion support") +
  scale_x_discrete(labels = c("List A", "List B", "Pooled"))
```

## Why?

- Same lists and random assignment throughout

. . .

- Differences could happen by chance

. . .

**OR** 

. . .

- Because Organization Y **stands out more** than Organization X

## DLE variants

```{r}
designs = data.frame(
  expand.grid(
    Lists = c("Fixed", "Randomized"),
    Sensitive_item = c("Fixed", "Randomized")
  )
)

colnames(designs) = c("List order", "Sensitive item location")

designs %>% 
  kbl(format = "markdown",
      escape = FALSE,
      align = "cc")
```


. . .

- **Fixed-fixed:** Not admissible

## DLE variants {visibility="uncounted"}

```{r}
designs %>% 
  kbl(format = "markdown",
      escape = FALSE,
      align = "cc")
```

- **Randomized-fixed:** Randomize list order, keep sensitive item in the second list to avoid contamination across lists

. . .

- But perhaps you want to detect contamination rather than avoid it

## DLE variants {visibility="uncounted"}

```{r}
designs %>% 
  kbl(format = "markdown",
      escape = FALSE,
      align = "cc")
```

- **Fixed-randomized:** Fixed list order, randomize sensitive item location

- **Randomized-randomized:** Randomize both

## DLE variants {visibility="uncounted"}

```{r}
designs %>% 
  kbl(format = "markdown",
      escape = FALSE,
      align = "cc")
```


- Last two let us make statistical uncertainty statements about whether discrepancy in estimates is systematic or random

## Intuition: Carryover design effects

::: incremental
- FR and RR designs have variation in **treatment schedules**

    - **Schedule 1:** See sensitive item in first list

    - **Schedule 2:** See sensitive item in second list
    
- If a sensitive item stands out, respondents may alter responses to baseline items in unintended ways

    - **Schedule 1:** Deflate responses to both lists
    
    - **Schedule 2:** Deflate responses to second list only
:::

## Example

::: incremental
- Seeing Organization Y may alert respondents to the researchers' interest on support for conservative organizations as a socially undesirable attitude

- If this is true, a conservative respondent would:

    - **Schedule 1:** Lie about supporting American Family Association **and** Tea Party Patriots
    
    - **Schedule 2:** Lie about supporting Tea Party Patriots only
    
- This is true even if they do not support Organization Y!

:::

## Statistical test: Difference in differences

::: incremental
- **Goal:** Distinguish carryover design effects from chance

- Ingredients:

    - $\widehat{\tau}_1$: Difference in means in Schedule 1
    - $\widehat{\tau}_2$: Difference in means in Schedule 2
    
- $H_0: \widehat{\tau}_1 = \widehat{\tau}_2$

- Compute p-values via randomization inference or regression

- Need to account for clustered data structure
:::

## Results

```{r}
load("attn_tests.RData")

dd = attn_tests %>% 
  filter(is.na(m)) %>%
  mutate_at(2:4, round, 3) %>% 
  select(!m) %>% 
  mutate(experiment = ifelse(experiment == "X", 
                             "Organization X",
                             "Organization Y"))

colnames(dd) = c("Experiment", "Statistic", "p-value")

dd %>% 
  kbl(format = "markdown")
```

. . .

- Little evidence against null for Organization X

- Some evidence against null for Organization Y, but $p>0.05$

. . .

- Test helps in detecting carryover design effects

## Additional considerations

- **Simulations:** Diff-in-diff performs worse as the correlation between baseline lists increases

- This is a problem because standard advice is to make correlated baseline lists ([Glynn 2013](https://doi.org/10.1093/poq/nfs070))

. . .

- **Alternative:** Signed rank test

- But signed rank has high false positive rate to detect response inflation

. . .

- **Suggestion:** Prefer sensitive items that are frowned upon

- **Example:** Voted "No" over voted "Yes"

## Conclusion

- Tests facilitate implementation of DLEs

- The cost of adding an extra question is not trivial

- Navigate research design choices at pilot stage

. . .

- Compatible with other innovations to enhance statistical precision

- Using auxiliary information ([Aronow et al 2015](https://doi.org/10.1093/jssam/smu023); [Chou et al 2020](https://doi.org/10.1177/0049124117729711))

- Placebo sensitive items ([Riambau and Ostwald 2021](https://doi.org/10.1017/psrm.2020.18))

::: {style="text-align: center"}
## Thank you! {.center background-color="#FDBF57"}

Feedback: <diazg2@mcmaster.ca>

Slides: <https://gustavodiaz.org/slides/dle/spark.html>

Paper: <https://gustavodiaz.org/files/research/dle_test.pdf>
:::


